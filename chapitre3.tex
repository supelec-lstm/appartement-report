\chapter{Graphe de calculs}

\section{Motivations}

Nous avons vu précédemment que l'architecture du réseau de neurones peut être lourde autant du point de vue modélisation que du point de vue calcul. Pour motiver l'apparition des graphes de calcul prenons l'exemple d'un réseau de neurones à $m$ entrées, une couche cachée de $n$ neurones et $p$ sorties. Tous les neurones ont pour fonction d'activation une sigmoïde $\sigma$. Un tel réseau est représenté sur la figure \ref{reseau_3_5_3}.

\begin{figure}
\begin{center}
\input{images/chapter3/reseau_3_5_3.tex}
\end{center}
\caption{Réseau de neurones à 3 entrées, 5 neurones cachés et 3 neurones de sorties}
\label{reseau_3_5_3}
\end{figure}

Nous pourrions modéliser ce réseau de neurone en définissant $n+p$ neurones ayant chacun son vecteur poids mais une façon équivalente de modéliser ce système est de rassembler les poids des deux couches dans deux matrices $W_1 \in \mathbb{R}^{m \times n}$ et $W_2 \in \mathbb{R}^{n \times p}$. En notant $x \in \mathbb{R}^m$ le vecteur ligne des entrées du réseau, on a alors que la sortie est :
\begin{equation}
y = \sigma(\sigma(xW_1)W_2)
\label{eq_3_5_3}
\end{equation}
où la fonction $\sigma$ s'applique terme à terme. 

Sur la figure \ref{reseau_3_5_3}, à droite, est représentée cette formule de manière graphique. Un tel graphe sera appelé graphe de calculs.

Cette modélisation matricielle a de nombreux avantages. Tout d'abord, elle est plus compacte et simple à manipuler et à visualiser. Puis, elle a un avantage certain pour notre implémentation en Python car elle permet d'utiliser pleinement la librairie d'algèbre linéaire \texttt{numpy} ce qui accélerera grandement les calculs. Enfin, il est facile d'étendre la formule précédente au calcul de la sortie de plusieurs vecteurs d'entrées. En effet si $X \in \mathbb{R}^{N \times m}$ est une matrice contenant $N$ vecteurs d'entrées alors la sortie du réseau s'écrit :
$$Y = \sigma(\sigma(XW_1)W_2)$$

On aurait donc envie de s'affranchir de la structure du neurone pour seulement modéliser les opérations mathématiques effectuées par le réseau.

\section{Définitions}

\begin{definition}
Un graphe de calcul est un graphe représentant une fonction mathématique. Un n\oe{}ud de ce graphe représente soit une une variable, des poids ou une opération mathématique. Formellement, nous noterons $\mathcal{G}(V, A, V_{var}, (V_{op}, f), (V_{poids}, \theta))$ le graphe $\mathcal{G}$ dont les n\oe{}uds sont $V$, les arêtes $A$, $V_{var} \subset V$ les n\oe{}uds réprésentant une variable, $V_{op} \subset V$ ceux représentant une opération et $f$ les opérations en question, et $V_{poids} \subset V$ ceux représentant des poids et $\theta$ les poids associés.
\end{definition}

Dans cette définition, nous considérons les variables comme les arguments du graphe de calculs. En ce qui concerne, les réseaux de neurones, les entrées seront un n\oe{}ud représentant une variable. Les poids sont les paramètres de la fonction. Ce seront les poids des neurones par exemple. Nous n'imposons pour l'instant aucune limite aux opérations mathématiques pouvant être effectuées au sein des graphes de calculs.

Dans le graphe de calculs de la figure \ref{reseau_3_5_3}, le n\oe{}ud $x$ est une variable, les n\oe{}uds $W_1$ et $W_2$ sont des poids et les deux n\oe{}uds $\sigma$ représente l'opération sigmoïde appliquée terme à terme. Nous remarquerons que la sortie $y$ n'est pas explicitée par un n\oe{}ud. 

Les graphes de calculs offrent une très grande liberté. Il est possible de représenter avec ceux-ci une classe de fonctions bien plus grandes qu'avec des réseaux de neurones classiques.

Écrivons dès maintenant, l'algorithme de propagation. Afin de pouvoir être concis, nous devons ajouter des notations. Nous noterons l'arc allant de la sortie $k$ du n\oe{}ud $n_i$ vers l'entrée $l$ du n\oe{}ud $n_j$ comme un quadruplet $(n_i, k, n_j, l)$. De plus, nous noterons $in(n)$ le nombre d'entrées du n\oe{}ud $n$ et $out(n)$ le nombres de sorties du n\oe{}ud $n$.

\begin{algorithm} 
\begin{algorithmic}
\Procedure{$evaluer\_graphe$}{$\mathcal{G}(V, A, V_{var}, (V_{op}, f), (V_{poids}, \theta)), x$}
\Function{$evaluer\_noeud$}{$n_j$}
	\If{$déjàCalculé[j]$ est $faux$}
		\State $t \leftarrow (evaluer\_noeud(i)_k, (n_i, k, l)\text{ tel que }(n_i, k, n_j, l) \in A)$
		\State $y_j \leftarrow f_j(t)$
		\State $déjàCalculé[j] \leftarrow vrai$
	\EndIf
	\State \Return $y_j$
\EndFunction

\State Initialiser un tableau $dejaCalculé$ de longueur $|V|$ à $faux$.
\For{$n_i \in V_{var}$}
	\State $y_i \leftarrow x_i$
	\State $déjàCalculé[i] \leftarrow vrai$ 
\EndFor
\For{$n_j \in V$}
	\State $evaluer\_noeud(n_j)$ 
\EndFor
\EndProcedure
\end{algorithmic}
\caption{Algorithme d'évaluation d'un graphe de calculs.}
\label{propagation_memoisation}
\end{algorithm}

Nous remarquons que l'algorithme de propagation est très similaire à celui décrit pour les réseaux de neurones. En effet, les graphes de calculs en sont une généralisation.

Nous avons ainsi modéliser nos réseaux de manière plus efficace. Il reste cependant un problème à résoudre. Notre objectif est d'optimiser les paramètres de notre fonction. Pour cela, nous avons besoin de calculer la dérivée du coût $E$ par rapport aux poids avant de pouvoir utiliser l'algorithme de la descente du gradient.

\section{Dérivation automatique}

Afin de réaliser cela, nos n\oe{}uds représentant des opérations mathématiques ne seront pas seulement responsable de calculer une sortie mais aussi de propager le gradient. En effet prenons un n\oe{}ud représent une opération mathématique $f$ à $m$ entrées $x_1, ..., x_m$ et à $n$ sorties $y_1, ..., y_m$. La règle de la chaine, nous permet d'exprimer la dérivée du coût par rapport aux entrées en fonction du coût par rapport aux sorties du n\oe{}ud. En effet, on a :
\begin{equation}
\forall i \in \{1, ...,  m\}, \frac{\partial E}{\partial x_i} = \sum_{j=1}^{n}{\frac{\partial y_j}{\partial x_i}\frac{\partial E}{\partial y_j}} = \sum_{j=1}^{n}{\frac{\partial f_j}{\partial x_i}(x)\frac{\partial E}{\partial y_j}}
\label{retropropagation_graphe}
\end{equation}

Rappelons que si $x_i \in \mathbb{R}^p$ et $y_j \in \mathbb{R}^q$ alors $\frac{\partial E}{\partial y_j} \in \mathbb{R}^q$ et $\frac{\partial y_j}{\partial x_i} = (\frac{\partial y_{j,l}}{\partial x_{i, k}})_{k, l} \in \mathbb{R}^{p \times q}$.

La formule \ref{retropropagation_graphe} nous permet de décrire un algorithme de rétropropagation du gradient très similaire à celui des réseaux de neurones :

\begin{algorithm} 
\begin{algorithmic}
\Procedure{$retropropager\_gradient$}{$\mathcal{G}(V, A, V_{var}, (V_{op}, f), (V_{poids}, \theta)), x, y$}
\Function{$calculer\_gradient$}{$n_i$}
	\If{$déjàCalculé_j$ est $faux$}
		\For{k dans $\{1, ..., out(n_i)\}$}
			\State $\frac{\partial E}{\partial y_{i, k}} \leftarrow \sum_{(n_j, l)\text{ tel que }(n_i, k, n_j, l)}{calculer\_gradient(n_j)_l}$
		\EndFor
		\For{l dans $\{1, ..., out(n_i)\}$}
			\State $\frac{\partial E}{\partial x_{i, l}} \leftarrow \sum_{k \in \{1, ..., out(n_i) \}}{\frac{\partial f_{i, k}}{\partial x_{i, l}}(x)\frac{\partial E}{\partial y_{i, k}}}$
		\EndFor
		\State $déjàCalculé_j \leftarrow vrai$
	\EndIf
	\State \Return $\frac{\partial E}{\partial x_j}$
\EndFunction

\State Appeler $evaluer\_graphe(\mathcal{N}(V, A, V_{in}, V_{out}, f), x)$ et récupérer les entrées et sorties de chaque noeud
\State Initialiser un tableau $dejaCalculé$ de longueur $|V|$ à $faux$.
\For{$n_i \in V_{out}$}
	\State Calculer $\frac{\partial E}{\partial y_i}$
	\State $déjàCalculé_i \leftarrow vrai$ 
\EndFor
\For{$n_i \in V$}
	\State $calculer\_gradient(n_i)$ 
\EndFor
\EndProcedure
\end{algorithmic} 
\caption{Algorithme de rétropropagation du gradient dans un graphe de calculs.}
\label{propagation_memoisation2}
\end{algorithm}

\section{N\oe{}uds}

Maintenant que les graphes de calculs ont été présentés de manière théorique, nous allons dans la suite présenter de manière plus pratique comment nous les avons implémentés.

\subsection{La classe Node}

Un n\oe{}ud aura deux missions : calculer sa sortie et rétropropager le gradient. Nous représentions ces deux opérations sur la figure \ref{node}.

\begin{figure}
\begin{center}
\begin{tabular}{cc}
\input{images/chapter3/node_propagation.tex} & \input{images/chapter3/node_backpropagation.tex} \\
\end{tabular}
\end{center}
\label{node}
\end{figure}

Lors de la propagation, un n\oe{}ud demande ses entrées à ses parents puis calcule sa sortie et la mémoïse. De façon analogue, lors de la rétropropagation, un n\oe{}ud demande à chacun de ses enfants le gradient par rapport à la sortie qu'ils utilisent.

\subsection{Les opérations mathématiques}

Dans cette sous-section, nous allons décrire les symboles ainsi que les formules des n\oe{}uds que nous avons implémentés.

\begin{figure}[h!]
\begin{center}
\begin{tabular}{|m{4cm}|m{3.cm}|m{3.cm}|}
\hline
\begin{center}Symbole\end{center} & \begin{center}Formule de propagation\end{center} & \begin{center}Formule de rétropropagation\end{center} \\
\hline
\begin{center}\input{images/chapter3/nodes/node_sigmoid.tex}\end{center} & \begin{center}$\sigma(x) = \frac{1}{1 + \exp(-x)}$\end{center} & \begin{center}$\frac{\partial E}{\partial y} \odot y \odot (1 - y)$\end{center} \\
\hline
\begin{center}\input{images/chapter3/nodes/node_tanh.tex}\end{center} & \begin{center}$\tanh(x)$\end{center} & \begin{center}$\frac{\partial E}{\partial y} \odot (1 - y \odot y)$\end{center} \\ 
\hline
\end{tabular}
\end{center}
\caption{N\oe{}ud à une entrée et une sortie}
\end{figure}

\begin{figure}[h!]
\begin{center}
\begin{tabular}{|m{4cm}|m{3.cm}|m{3.cm}|}
\hline
\begin{center}Symbole\end{center} & \begin{center}Formule de propagation\end{center} & \begin{center}Formule de rétropropagation\end{center} \\
\hline
\begin{center}\input{images/chapter3/nodes/node_addition.tex}\end{center} & \begin{center}$x_1 + x_2$\end{center} & \begin{center}$(\frac{\partial E}{\partial y}, \frac{\partial E}{\partial y})$\end{center} \\
\hline
\begin{center}\input{images/chapter3/nodes/node_substraction.tex}\end{center} & \begin{center}$x_1 - x_2$\end{center} & \begin{center}$(\frac{\partial E}{\partial y}, -\frac{\partial E}{\partial y})$\end{center} \\
\hline
\begin{center}\input{images/chapter3/nodes/node_multiplication.tex}\end{center} & \begin{center}$x_1 \times x_2$\end{center} & \begin{center}$(\frac{\partial E}{\partial y}x_2^T, x_1^T\frac{\partial E}{\partial y})$\end{center} \\
\hline
\begin{center}\input{images/chapter3/nodes/node_ew_multiplication.tex}\end{center} & \begin{center}$x_1 \odot x_2$\end{center} & \begin{center}$(\frac{\partial E}{\partial y} \odot x_2, \frac{\partial E}{\partial y} \odot x_1)$\end{center} \\
\hline
\end{tabular}
\end{center}
\caption{N\oe{}ud à deux entrées et une sortie}
\end{figure}

\subsection{Démonstrations}

\section{La classe Graph}


\section{Diagramme UML}


\section{Conclusion}

L'architecture en Graphe de Calcul donne de très bons résultats du point de vue de la rapidité de calcul. On observe par exemple une réduction d'un facteur 1000 du temps de calcul pour l'entrainement du réseau simple sur l'ensemble MNIST par rapport à l'implémentation "naïve" du chapitre 2. Ces performances sont atteintes grâce à la diminution du nombre d'appels objets, remplacés par des calculs matriciels sur des matrices de grande taille gérées par des bibliothèques efficaces.