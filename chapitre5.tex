\chapter{Réseaux de neurones récurrents}

\section{Motivation}

Nous avons pu voir que les réseaux feedforward peuvent être utilisés afin d'appendre à prédire la sortie voulue en fonction de l'entrée du réseau. 

Néanmoins, ces réseaux sont limités à des entrées et des sorties de tailles fixes. De plus il ne peuvent gérer des réseaux présentant des cycles. On utilise alors des réseaux récurrents qui permettent un traitement plus efficace des séquences de données. On pourra ainsi faire de la prédiction et de la génération de séquence.

Dans un premier, nous verrons comment modéliser ces réseaux récurrents. Puis nous nous intéresserons à deux algorithmes d'optimisation permettant l'apprentissage des réseaux récurrents : real time recurrent learning (RTRL) et backpropagation through time (BPTT).

\section{Dépliage}

Nous avons dit précédemment qu'un réseau de neurones récurrent est un réseau possédant des cycles. Néanmoins, il est difficile d'imaginer que la valeur de sortie d'un neurone au temps $t$ dépende de la sortie de ce même neurone au temps $t$. Pour résoudre ce problème, nous allons ajouter une dimension temporelle à nos réseaux et permettre aux neurones au temps $t$ de dépendre des valeurs d'autres neurones au temps $t-1$.

Afin de mettre en évidence cette dépendance temporelle sur nos graphes, les arêtes reliant la sortie d'un neurone au temps $t-1$ à un neurone au temps $t$ vont être annotées par un carré comme sur la figure \ref{arete_retard}. On les appelera arête "retard".

\begin{figure}
\begin{center}
\input{images/chapter5/recurrent_edge.tex}
\caption{Réseau de neurones récurrent où l'arête avec un carré signifie que le n\oe{}ud $h$ prend sa valeur au temps $t-1$ comme entrée au temps $t$.}
\label{arete_retard}
\end{center}
\end{figure}

On peut maintenant remarquer que pour une séquence $x_1, ..., x_n$ le réseau récurrent de la figure \ref{arete_retard} est équivalent au réseau feedforward de la figure \ref{reseau_deplie}.

\begin{figure}
\begin{center}
\input{images/chapter5/unfolded_graph.tex}
\caption{Réseau de neurones feedforward équivalent au réseau récurrent de la figure \ref{arete_retard} pour la séquence d'entrée $x_1, ..., x_n$.}
\label{reseau_deplie}
\end{center}
\end{figure}

Le processus de passage d'un réseau récurrent à un réseau feedforward s'appelle dépliage.

Une fois le réseau déplié obtenu, on peut utiliser l'algorithme de propagation classique pour les graphes de calculs afin de calculer la sortie du réseau récurrent.

Nous avons utilisé trois méthodes pour générer des réseaux dépliés. La première prend en entrée un réseau récurrent avec des arêtes retards et obtient le dépliage à partir de celui-ci.

La seconde prend en entrée un \textit{layer}, représenté figure \ref{layer}, c'est-à-dire le sous-graphe qui va être copié à chaque pas de temps et le clone autant de fois qu'il le faut.

\begin{figure}
\begin{center}
\input{images/chapter5/layer.tex}
\caption{Représentation d'un layer. Il contient les n\oe{}uds du graphe à un temps $t$. Il prend en entrée la sortie de l'état caché du temps $t-1$ appelée $h_{in}$ et l'entrée $x_t$. Il a comme sortie l'état caché $h_{out}$ ainsi que $\hat{y}_t$ qui sera comparé avec $y_t$ la sortie attendue au temps $t$. Le $layer$ sert aussi lors de la phase de rétropropagation.}
\label{layer}
\end{center}
\end{figure}

Enfin la dernière méthode est de ne pas utiliser d'algorithme "général" permettant de passer d'un graphe récurrent à un graphe acyclique. Mais plutôt d'écrire pour chaque architecture de réseau une fonction dépliant ce réseau. Une telle fonction contiendra principalement une boucle créant les n\oe{}uds nécessaires pour chaque temps.

Devant la diversité des architectures des réseaux récurrents, la dernière méthode est la plus sensée et la plus pratique. Elle offre une liberté totale quant à l'architecture du graphe tout en étant relativement simple.

\section{BPTT}

Contrairement, à ce que nous avons fait lors du projet, nous allons d'abord présenter l'algorithme \textit{backpropagation through time} (BPTT) car il s'agit d'une simple généralisation de la rétropropagation déjà présentée pour les réseaux feedforward.

L'algorithme consiste en deux étapes :
\begin{enumerate}
\item Déplier le réseau récurrent pour avoir un réseau feedforward.
\item Appliquer l'algorithme de rétropropagation à ce réseau feedforward.
\end{enumerate}

La seule difficulté conceptuelle réside donc dans le dépliage.

En pratique, le dépliage est un processus coûteux en temps. Lors de l'apprentissage, on évite donc de déplier un graphe pour chaque exemple d'apprentissage. Pour cela, si cela est possible, on fixe la taille des séquences d'entrée lors de l'apprentissage. Nous pourrons ainsi utiliser le même réseau pour tous les exemples. En outre, comme tous les exemples auront la même taille, nous pourrons les placer dans une matrice comme pour les réseaux feedforward et passer un batch d'exemples en même temps ce qui permet d'accélérer encore les calculs en utilisant pleinement la puissance de \texttt{numpy}.

\section{RTRL}

\label{RTRL_section}

Détaillons maintenant l'algorithme RTRL qui a une philosophique complètement différente. Au lieu de rétropropager le gradient, nous allons le propager.

Nous nous plaçons dans le cas particulier où le réseau est fully connected comme sur la figure \ref{fully_connected_recurrent_network}. 

\begin{figure}
\begin{center}
\input{images/chapter5/fully_connected_recurrent_network.tex}
\caption{Réseau de neurones récurrent fully connected avec 2 entrées et 3 autres neurones.}
\label{fully_connected_recurrent_network}
\end{center}
\end{figure}

Nous notons $I$ l'ensemble des neurones d'entrée et $U$ l'ensemble des autres neurones. Posons alors une notation unifiant ces deux ensembles : 
\begin{equation}
z_k(t) = \left\{
    \begin{array}{ll}
        x_k(t) \text{ si } k \in I \\
        y_k(t-1) \text{ si } k \in U
    \end{array}
\right.
\label{z_k}
\end{equation}
avec $y_k(0) = 0$ pour tout $k \in U$.

Posons aussi, $s_k(t)$ la somme pondérée des entrées du neurone $k$ au temps $t$ :
\begin{equation}
s_k(t) = \sum_{l \in U \cup I}{w_{k,l}z_l(t)}
\label{s_k}
\end{equation}

On a alors :
$$
\forall k \in U, y_k(t) = f_k(s_k(t))
$$

La fonction de coût total est la somme des coûts partiels que l'on calcule à chaque temps $t$ :
$$
E = \sum_{t=1}^{n}{E(t)}
$$

Nous pouvons alors calculer la dérivée du coût partiel au temps $t$, $E(t)$, par rapport à un poids $w_{i,j}$ :
\begin{equation}
\frac{\partial E(t)}{\partial w_{i,j}} = \sum_{k \in U}{\frac{\partial E(t)}{\partial y_k(t)}\frac{\partial y_k(t)}{\partial w_{i,j}}}
\label{E_t}
\end{equation}
Le terme $\frac{\partial E(t)}{\partial y_k(t)}$ est connu, il dépend de la fonction de coût utilisée. Pour une fonction de coût quadratique $E(t) = \sum_{k \in U}{(y_k(t) - d_k(t))^2}$ avec $d_k(t)$ la valeur attendue pour $y_k(t)$, on a $\frac{\partial E(t)}{\partial y_k(t)} = 2(y_k(t) - d_k(t))$. 

Il nous reste donc le terme $\frac{\partial y_k(t)}{\partial w_{i,j}}$ à calculer. Déterminons une relation de récurrence entre $\frac{\partial y_k(t+1)}{\partial w_{i,j}}$ et les $(\frac{\partial y_l(t)}{\partial w_{i,j}})_{l\in U}$ :
$$
\frac{\partial y_k(t+1)}{\partial w_{i,j}} 
= \frac{\partial y_k(t+1)}{\partial s_k(t+1)}\frac{\partial s_k(t+1)}{\partial w_{i,j}}
= f_k'(s_k(t+1))\frac{\partial s_k(t+1)}{\partial w_{i,j}}
$$
En utilisant la formule \ref{s_k}, on a alors :
$$
\begin{array}{rcl}
\frac{\partial y_k(t+1)}{\partial w_{i,j}} & 
= & f_k'(s_k(t+1))\sum_{l \in I \cup U}{\frac{\partial s_k(t+1)}{\partial z_l(t+1)}\frac{\partial z_l(t+1)}{\partial w_{i,j}}+\frac{\partial s_k(t+1)}{\partial w_{k,l}}\frac{\partial w_{k,l}}{\partial w_{i,j}}} \\
& = & f_k'(s_k(t+1))\left(\sum_{l \in I \cup U}{w_{k,l}\frac{\partial z_l(t+1)}{\partial w_{i,j}}}+\delta_{i,k}z_j(t+1)\right)
\end{array}
$$
Puis en utilisant la formule \ref{z_k}, on a finalement :
\begin{equation}
\frac{\partial y_k(t+1)}{\partial w_{i,j}} = f_k'(s_k(t+1))\left(\sum_{l \in U}{w_{k,l}\frac{\partial y_l(t)}{\partial w_{i,j}}}+\delta_{i,k}z_j(t+1)\right)
\label{y_k_recurrence}
\end{equation}

Utilisons les formules \ref{E_t} et \ref{y_k_recurrence} pour écrire l'algorithme de propagation du gradient. Dans l'algorithme \ref{RTRL}, $p$ représente les $(\frac{\partial y_k(t)}{\partial w_{i,j}})_{k,i,j}$ et $\epsilon$ les $(\frac{\partial E}{\partial w_{i,j}})_{i,j}$. On effectue la propagation des entrées en même temps que la propagation du gradient.

\begin{algorithm} 
\begin{algorithmic}
\Procedure{$propager\_gradient$}{$I, U, w, f, x, d$}
\State Initialiser un tableau $z$ à 2 dimensions de taille $(n+1, |I \cup U|)$ à $0$.
\State Initialiser un tableau $s$ de taille $|U|$ à $0$.
\State Initialiser un tableau $p$ à 3 dimensions de taille $(|U|, |U|, |I \cup U|)$ à $0$.
\State Initialiser un tableau $\epsilon$ à 2 dimensions de taille $(|U|, |I \cup U|)$ à $0$.
\For{$t \in \{1, ..., n\}$}
	\State // Propagation
	\For{$k \in I$}
		\State $z[t-1][k] \leftarrow x[t][k]$
	\EndFor
	\For{$k \in U$}
		\State $s[k] \leftarrow \sum_{l \in U \cup I}{w_{k,l}z[t-1][l]}$
		\State $z[t][k] \leftarrow f_k(s[k])$
	\EndFor
	\State // Propagation de l'erreur
	\For{$k \in U$}
		\For{$i \in U$}
			\For{$j \in I \cup U$}
				\State $p[k][i][j] \leftarrow f_k'(s[k])\left(\sum_{l \in U}{w_{k,l}p[l-1][i][j]+\delta_{i,k}z[t][j]}\right)$
			\EndFor
		\EndFor
	\EndFor
	\For{$i \in U$}
		\For{$j \in I \cup U$}
			\State $\epsilon[i][j] \leftarrow \epsilon[i][j] + \sum_{k \in U}{2(z[t][k]-d[t][k])p[k][i][l]}$
		\EndFor
	\EndFor
\EndFor
\EndProcedure
\end{algorithmic} 
\caption{Algorithme RTRL.}
\label{RTRL}
\end{algorithm}

On parle d'apprentissage en temps réel car il est possible d'apprendre après élément de la séquence $x_1, ..., x_n$. Ou bien d'accumuler le gradient sur toute la séquence $x_1, ..., x_n$ comme présenté dans l'algorithme \ref{RTRL}. Nos expériences ont montré qu'il était mieux d'accumuler le gradient sur une séquence en entière voire sur des batches de plusieurs séquences.

\section{Évaluation}

\subsection{Grammaire de Reber}

Dans un premier temps, nous comparerons ces algorithmes sur l'apprentissage de la grammaire de Reber, qui est régulière. Nous utiliserons la grammaire simple présentée sur la figure \ref{Grammaire de Reber simple}.

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.3]{images/reber_simple.png}
\caption{Automate reconnaissant la grammaire de Reber simple}
\label{Grammaire de Reber simple}
\end{center}
\end{figure}

A l'aide de cette automate, nous générerons deux ensembles de mots respectivement afin d'entraîner et de tester notre futur réseau. L'objectif du réseau sera alors d'apprendre cette grammaire afin de prédire les caractères possibles après un caractère donné.

L'étape suivante consistera à tester les algorithmes sur l'apprentissage de la grammaire de Reber symmétrique présentée sur la figure \ref{Grammaire de Reber symétrique}.  
\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.5]{images/reber_symmetrique.png}
\caption{Automate reconnaissant la grammaire de Reber symmétrique}
\label{Grammaire de Reber symmétrique}
\end{center}
\end{figure}

Dans cette situation, le réseau devra être capable de se souvenir d'un état passé (T ou P) afin de prédire le nouvel état (T ou P). Cet état passé pourra être plus ou moins lointain en fonction de la taille de la grammaire de Reber insérée au milieu. 

\section{Comparaison des deux algorithmes}

\section{Problème des dépendances temporelles}

\label{dependances_temporelles}

Pour conclure cette partie, nous allons effectuer une analyse théorique montrant les limitations des réseaux récurrents fully-connected. Il en resortira la nécessité d'une architecture plus complexe permettant la conservation des dépendances temporelles à long terme.

Nous allons prendre un réseau et des notations similaires à celui utilisé dans la partie \ref{RTRL_section} sur l'algorithme RTRL. On retrouvera une visualisation de l'architecture considérée dans la figure \ref{archi_dep_tempo}. On prend un réseau avec $n$ neurones cachés numérotés de $1$ à $n$, $m - n$ neurones de sortie numérotés de $n+1$ à $m$ et $p - m$ neurones d'entrée numérotés de $m+1$ à $p$.

Les neurones cachés prennent comme entrées au temps $t$ les entrées du réseau au temps $t$ ainsi que les sorties des neurones cachés au temps $t-1$. Tandis que les neurones de sortie prennent comme entrées les sorties des neurones cachés au temps $t$.

On note alors pour $i \in \{1, ..., m\}$ :
\begin{itemize}
	\item $s_i(t) = \left\{
    \begin{array}{ll}
        \sum_{j = 1}^{n}{w_{ij}y_j(t-1)} + \sum_{j = m+1}^{p}{w_{ij}y_j(t)} & \textsf{si } i \in \{1, ..., n\} \\
        \sum_{j = 1}^{n}{w_{ij}y_j(t)} & \textsf{si } i \in \{n+1, ..., m\}
    \end{array}
\right.$
	\item $y_i(t) = f_i(s_i(t))$
	\item $E = \sum{E(t)}$
	\item $\epsilon_i(t) = \frac{\partial E}{\partial s_i(t)}$
\end{itemize}

\begin{figure}
\begin{center}
\input{images/chapter5/network_demo_propag.tex}
\caption{Architecture considérée dans la sous-section \ref{dependances_temporelles}}
\label{archi_dep_tempo}
\end{center}
\end{figure}

Notre objectif est de montrer que la dépendance temporelle entre un neurone caché du temps $t$ et d'un neurone du temps $t + q$ diminue rapidement avec $q$. Ce qui montrera que cette architecture n'est pas adaptée pour apprendre des dépendances à long terme.

Pour cela, nous allons étudier la quantité $\frac{\partial \epsilon_i(t)}{\partial \epsilon_j(t+q)} = \frac{\frac{\partial E}{\partial s_i(t)}}{\frac{\partial E}{\partial s_j(t+q)}}$ qui peut se comprendre intuitivement en simplifiant abusivement l'expression en $\frac{\partial s_j(t+q)}{\partial s_i(t)}$ c'est-à-dire la variation de l'activation d'un neurone $j$ du temps $t+q$ par rapport à celle d'un neurone $i$ du temps $t$. Nous allons montrer que cette quantité décroit rapidement avec $q$.

Pour un neurone de sortie, on a :
$$
\epsilon_i(t) = \frac{\partial E}{\partial y_i(t)}\frac{\partial y_i(t)}{s_i(t)} = \frac{\partial E}{\partial y_i(t)}f'_i(s_i(t))
$$

De même pour un neurone caché, d'après la règle de la chaine :
$$
\epsilon_i(t) = \sum_{j = 0}^{n}{\frac{\partial E}{\partial s_j(t+1)}\frac{\partial s_j(t+1)}{\partial s_i(t)}} + \sum_{j = n+1}^{m}{\frac{\partial E}{\partial s_j(t)}\frac{\partial s_j(t)}{\partial s_i(t)}}
$$

D'où :
$$
\epsilon_i(t) = \sum_{j = 0}^{n}{\epsilon_j(t+1)f'_i(s_i(t))w_{ji}} + \sum_{j = n+1}^{m}{\epsilon_j(t)f'_i(s_i(t))w_{ji}}
$$

On en déduit :
$$
\frac{\partial \epsilon_i(t)}{\partial \epsilon_j(t+1)} = f'_i(s_i(t))w_{ji}
$$

D'où par récurrence :
$$
\frac{\partial \epsilon_i(t)}{\partial \epsilon_j(t+q)} = \sum_{l_1 = 0}^{n}{\sum_{l_2 = 0}^{n}{...\sum_{l_{q-1} = 0}^{n}{\prod_{i=1}^{q}{f'_{l_i}(t+i-1)w_{l_{i+1}l_i}}}}}
$$
Cette quantité représente un flux local.

Réécrivons cette quantité en utilisant une notation matricielle :
$$
\frac{\partial \epsilon_i(t)}{\partial \epsilon_j(t+q)} = W_j^TF'(t + q - 1)(\prod_{k = 2}^{q-1}{WF'(t + q - k)})W_if'_i(s_i(t))
$$
avec : 
\begin{itemize}
\item $W = (w_{ij})_{i,j}$
\item $F'(t) = diag(f'_1(s_1(t)), ..., f'_n(s_n(t)))$
\end{itemize}

Soit une norme matricielle $||.||_A$ compatible avec une norme vectorielle $||.||_x$ telle que $max_{1, ..., n}(|x_i|) \leq ||x||_x$.

L'hypothèse sur la norme $||.||_x$ donne :
$$
|x^Ty| = |\sum_{i=1}^{n}{x_iy_i}| \leq n max(|x_i|) max(|y_i|) \leq n||x||_x ||y||_x
$$

On a alors :
$$
|\frac{\partial \epsilon_i(t)}{\partial \epsilon_j(t+q)}| \leq n ||W_j||_x ||F'(t + q - 1)(\prod_{k = 2}^{q-1}{WF'(t + q - k)})W_if'_i(s_i(t))||_x
$$

Par compatibilité :
$$
|\frac{\partial \epsilon_i(t)}{\partial \epsilon_j(t+q)}| \leq n ||W_j||_x ||W||_A^{q-2} ||W_i||_x (f'_{max})^{q}
$$
avec $f'_{max} = max_{1, ..., n}(||F'(i)||_A)$

On remarque que :
$$
\forall i, ||W_i||_x = ||We_i||_x \leq ||W||_A ||e_i||_x \leq \lambda ||W||_A
$$
avec $e_i=(1_{i=j})_j$ et $\lambda = max_{1, ..., n}(||e_i||_x)$

Finalement :
$$
|\frac{\partial \epsilon_i(t)}{\partial \epsilon_j(t+q)}| \leq \lambda n ||W||_A^{q}(f'_{max})^{q}
$$

On prend $||A||_A = max_i(\sum_{j}{|a_{ij}|})$, $||x||_x = max_i(|x_i|)$ et $f_i(t) = \sigma(t) = \frac{1}{1+exp(-t)}$. On a alors $f'_{max} = \frac{1}{4}$, $\lambda = 1$ et $||W||_A \leq n w_{max}$ avec $w_{max} = max_{i,j}(|w_{ij}|)$, d'où :
$$
|\frac{\partial \epsilon_i(t)}{\partial \epsilon_j(t+q)}| \leq n (\frac{nw_{max}}{4})^{q}
$$

On en conclut que si $w_{max} < \frac{4}{n}$ alors on a une décroissance exponentielle.

Ce résultat dit donc que si les poids sont inférieurs à une certaine valeur alors, on peut être sûr que le réseau ne pourra pas modéliser convenablement les dépendances à long terme.